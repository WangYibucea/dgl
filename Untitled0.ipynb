{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFNlBVxZQyJIabawZ3pWEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WangYibucea/dgl/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-lXq6My0o0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "dc58311d-c3c2-4596-c445-43a13615f451"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "from time import time\n",
        "import shutil\n",
        "import argparse\n",
        "import configparser\n",
        "from ASTGCN_r import make_model\n",
        "from lib.utils import load_graphdata_channel1, get_adjacency_matrix, compute_val_loss_mstgcn, predict_and_save_results_mstgcn\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--config\", default='configurations/PEMS04_astgcn.conf', type=str,\n",
        "                    help=\"configuration file path\")\n",
        "args = parser.parse_args()\n",
        "config = configparser.ConfigParser()\n",
        "print('Read configuration file: %s' % (args.config))\n",
        "config.read(args.config)\n",
        "data_config = config['Data']\n",
        "training_config = config['Training']\n",
        "\n",
        "adj_filename = data_config['adj_filename']\n",
        "graph_signal_matrix_filename = data_config['graph_signal_matrix_filename']\n",
        "if config.has_option('Data', 'id_filename'):\n",
        "    id_filename = data_config['id_filename']\n",
        "else:\n",
        "    id_filename = None\n",
        "\n",
        "num_of_vertices = int(data_config['num_of_vertices'])\n",
        "points_per_hour = int(data_config['points_per_hour'])\n",
        "num_for_predict = int(data_config['num_for_predict'])\n",
        "len_input = int(data_config['len_input'])\n",
        "dataset_name = data_config['dataset_name']\n",
        "\n",
        "model_name = training_config['model_name']\n",
        "\n",
        "# ctx = training_config['ctx']\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = ctx\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device('cuda:0')\n",
        "print(\"CUDA:\", USE_CUDA, DEVICE)\n",
        "\n",
        "learning_rate = float(training_config['learning_rate'])\n",
        "epochs = int(training_config['epochs'])\n",
        "start_epoch = int(training_config['start_epoch'])\n",
        "batch_size = int(training_config['batch_size'])\n",
        "num_of_weeks = int(training_config['num_of_weeks'])\n",
        "num_of_days = int(training_config['num_of_days'])\n",
        "num_of_hours = int(training_config['num_of_hours'])\n",
        "time_strides = num_of_hours\n",
        "nb_chev_filter = int(training_config['nb_chev_filter'])\n",
        "nb_time_filter = int(training_config['nb_time_filter'])\n",
        "in_channels = int(training_config['in_channels'])\n",
        "nb_block = int(training_config['nb_block'])\n",
        "K = int(training_config['K'])\n",
        "\n",
        "folder_dir = '%s_h%dd%dw%d_channel%d_%e' % (model_name, num_of_hours, num_of_days, num_of_weeks, in_channels, learning_rate)\n",
        "print('folder_dir:', folder_dir)\n",
        "params_path = os.path.join('experiments', dataset_name, folder_dir)\n",
        "print('params_path:', params_path)\n",
        "\n",
        "\n",
        "train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel1(\n",
        "    graph_signal_matrix_filename, num_of_hours,\n",
        "    num_of_days, num_of_weeks, DEVICE, batch_size)\n",
        "\n",
        "adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename)\n",
        "\n",
        "net = make_model(DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, time_strides, adj_mx,\n",
        "                 num_for_predict, len_input, num_of_vertices)\n",
        "\n",
        "\n",
        "def train_main():\n",
        "    if (start_epoch == 0) and (not os.path.exists(params_path)):\n",
        "        os.makedirs(params_path)\n",
        "        print('create params directory %s' % (params_path))\n",
        "    elif (start_epoch == 0) and (os.path.exists(params_path)):\n",
        "        shutil.rmtree(params_path)\n",
        "        os.makedirs(params_path)\n",
        "        print('delete the old one and create params directory %s' % (params_path))\n",
        "    elif (start_epoch > 0) and (os.path.exists(params_path)):\n",
        "        print('train from params directory %s' % (params_path))\n",
        "    else:\n",
        "        raise SystemExit('Wrong type of model!')\n",
        "\n",
        "    print('param list:')\n",
        "    print('CUDA\\t', DEVICE)\n",
        "    print('in_channels\\t', in_channels)\n",
        "    print('nb_block\\t', nb_block)\n",
        "    print('nb_chev_filter\\t', nb_chev_filter)\n",
        "    print('nb_time_filter\\t', nb_time_filter)\n",
        "    print('time_strides\\t', time_strides)\n",
        "    print('batch_size\\t', batch_size)\n",
        "    print('graph_signal_matrix_filename\\t', graph_signal_matrix_filename)\n",
        "    print('start_epoch\\t', start_epoch)\n",
        "    print('epochs\\t', epochs)\n",
        "\n",
        "    criterion = nn.MSELoss().to(DEVICE)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    sw = SummaryWriter(logdir=params_path, flush_secs=5)\n",
        "    print(net)\n",
        "\n",
        "    print('Net\\'s state_dict:')\n",
        "    total_param = 0\n",
        "    for param_tensor in net.state_dict():\n",
        "        print(param_tensor, '\\t', net.state_dict()[param_tensor].size())\n",
        "        total_param += np.prod(net.state_dict()[param_tensor].size())\n",
        "    print('Net\\'s total params:', total_param)\n",
        "\n",
        "    print('Optimizer\\'s state_dict:')\n",
        "    for var_name in optimizer.state_dict():\n",
        "        print(var_name, '\\t', optimizer.state_dict()[var_name])\n",
        "\n",
        "    global_step = 0\n",
        "    best_epoch = 0\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    if start_epoch > 0:\n",
        "\n",
        "        params_filename = os.path.join(params_path, 'epoch_%s.params' % start_epoch)\n",
        "\n",
        "        net.load_state_dict(torch.load(params_filename))\n",
        "\n",
        "        print('start epoch:', start_epoch)\n",
        "\n",
        "        print('load weight from: ', params_filename)\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        params_filename = os.path.join(params_path, 'epoch_%s.params' % epoch)\n",
        "\n",
        "        val_loss = compute_val_loss_mstgcn(net, val_loader, criterion, sw, epoch)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(net.state_dict(), params_filename)\n",
        "            print('save parameters to file: %s' % params_filename)\n",
        "\n",
        "        net.train()  # ensure dropout layers are in train mode\n",
        "\n",
        "        for batch_index, batch_data in enumerate(train_loader):\n",
        "\n",
        "            encoder_inputs, labels = batch_data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(encoder_inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            training_loss = loss.item()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            sw.add_scalar('training_loss', training_loss, global_step)\n",
        "\n",
        "            if global_step % 1000 == 0:\n",
        "\n",
        "                print('global step: %s, training loss: %.2f, time: %.2fs' % (global_step, training_loss, time() - start_time))\n",
        "\n",
        "    print('best epoch:', best_epoch)\n",
        "\n",
        "    # apply the best model on the test set\n",
        "    predict_main(best_epoch, test_loader, test_target_tensor, _mean, _std, 'test')\n",
        "\n",
        "\n",
        "def predict_main(global_step, data_loader, data_target_tensor, _mean, _std, type):\n",
        "    '''\n",
        "\n",
        "    :param global_step: int\n",
        "    :param data_loader: torch.utils.data.utils.DataLoader\n",
        "    :param data_target_tensor: tensor\n",
        "    :param mean: (1, 1, 3, 1)\n",
        "    :param std: (1, 1, 3, 1)\n",
        "    :param type: string\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    params_filename = os.path.join(params_path, 'epoch_%s.params' % global_step)\n",
        "    print('load weight from:', params_filename)\n",
        "\n",
        "    net.load_state_dict(torch.load(params_filename))\n",
        "\n",
        "    predict_and_save_results_mstgcn(net, data_loader, data_target_tensor, global_step, _mean, _std, params_path, type)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_main()\n",
        "\n",
        "    predict_main(31, test_loader, test_target_tensor, _mean, _std, 'test')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-941f3df82d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mASTGCN_r\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_graphdata_channel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_adjacency_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_val_loss_mstgcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_and_save_results_mstgcn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ASTGCN_r'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}